import fetch from 'node-fetch'

let handler = async (m, { text, conn }) => {
  try {
    if (!text) return conn.reply(m.chat, 'ğŸ’­ *Ejemplo:* .bot Â¿CÃ³mo estÃ¡s?', m)

    await conn.sendMessage(m.chat, { react: { text: 'ğŸ¤”', key: m.key } })

    // ğŸ”— Nueva URL funcional de HuggingFace
    const response = await fetch("https://router.huggingface.co/hf-inference/models/mistralai/Mixtral-8x7B-Instruct-v0.1", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        inputs: text,
        parameters: { max_new_tokens: 100, temperature: 0.7 }
      })
    })

    // Si la respuesta no es vÃ¡lida
    if (!response.ok) {
      throw new Error(`âŒ FallÃ³ la conexiÃ³n (${response.status})`)
    }

    const result = await response.json()
    let output = result?.[0]?.generated_text || "âš ï¸ No pude pensar en una buena respuesta ahora mismo..."

    // âœ¨ Respuesta limpia
    const mensaje = `ğŸ’¬ *Respuesta IA:*\n${output.trim()}`
    await conn.reply(m.chat, mensaje, m)
    await conn.sendMessage(m.chat, { react: { text: 'âœ…', key: m.key } })

  } catch (e) {
    console.error('âŒ Error en .bot:', e)
    await conn.sendMessage(m.chat, { react: { text: 'âš ï¸', key: m.key } })
    await conn.reply(m.chat, 'âš ï¸ OcurriÃ³ un error al procesar tu mensaje.', m)
  }
}

handler.help = ['bot <texto>']
handler.tags = ['ia']
handler.command = ['bot', 'ia', 'ask']
export default handler
